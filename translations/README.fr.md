<div align="center">

<img src="/docs/logo.png" alt="VideoLingo Logo" height="140">

# Connecter le Monde, Image par Image

<a href="https://trendshift.io/repositories/12200" target="_blank"><img src="https://trendshift.io/api/badge/repositories/12200" alt="Huanshere%2FVideoLingo | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a>

[**English**](/README.md)ï½œ[**ç®€ä½“ä¸­æ–‡**](/translations/README.zh.md)ï½œ[**ç¹é«”ä¸­æ–‡**](/translations/README.zh-TW.md)ï½œ[**æ—¥æœ¬èª**](/translations/README.ja.md)ï½œ[**EspaÃ±ol**](/translations/README.es.md)ï½œ[**Ğ ÑƒÑÑĞºĞ¸Ğ¹**](/translations/README.ru.md)ï½œ[**FranÃ§ais**](/translations/README.fr.md)

</div>

## ğŸŒŸ AperÃ§u ([Essayez VL maintenant !](https://videolingo.io))

VideoLingo est un outil tout-en-un de traduction, de localisation et de doublage vidÃ©o visant Ã  gÃ©nÃ©rer des sous-titres de qualitÃ© Netflix. Il Ã©limine les traductions automatiques rigides et les sous-titres multi-lignes tout en ajoutant un doublage de haute qualitÃ©, permettant le partage des connaissances Ã  l'Ã©chelle mondiale au-delÃ  des barriÃ¨res linguistiques.

FonctionnalitÃ©s principales :
- ğŸ¥ TÃ©lÃ©chargement de vidÃ©os YouTube via yt-dlp

- **ğŸ™ï¸ Reconnaissance de sous-titres au niveau des mots et Ã  faible illusion avec WhisperX**

- **ğŸ“ Segmentation des sous-titres basÃ©e sur le NLP et l'IA**

- **ğŸ“š Terminologie personnalisÃ©e + gÃ©nÃ©rÃ©e par IA pour une traduction cohÃ©rente**

- **ğŸ”„ Processus en 3 Ã©tapes : Traduction-RÃ©flexion-Adaptation pour une qualitÃ© cinÃ©matographique**

- **âœ… Sous-titres uniquement sur une ligne, aux normes Netflix**

- **ğŸ—£ï¸ Doublage avec GPT-SoVITS, Azure, OpenAI et plus**

- ğŸš€ DÃ©marrage et traitement en un clic dans Streamlit

- ğŸŒ Support multi-langues dans l'interface utilisateur Streamlit

- ğŸ“ Journalisation dÃ©taillÃ©e avec reprise de la progression

DiffÃ©rence par rapport aux projets similaires : **Sous-titres sur une seule ligne uniquement, qualitÃ© de traduction supÃ©rieure, expÃ©rience de doublage transparente**

## ğŸ¥ DÃ©mo

<table>
<tr>
<td width="33%">

### Sous-titres Doubles
---
https://github.com/user-attachments/assets/a5c3d8d1-2b29-4ba9-b0d0-25896829d951

</td>
<td width="33%">

### Clonage Vocal Cosy2
---
https://github.com/user-attachments/assets/e065fe4c-3694-477f-b4d6-316917df7c0a

</td>
<td width="33%">

### GPT-SoVITS avec ma voix
---
https://github.com/user-attachments/assets/47d965b2-b4ab-4a0b-9d08-b49a7bf3508c

</td>
</tr>
</table>

### Support des langues

**Support des langues d'entrÃ©e (d'autres Ã  venir) :**

ğŸ‡ºğŸ‡¸ Anglais ğŸ¤© | ğŸ‡·ğŸ‡º Russe ğŸ˜Š | ğŸ‡«ğŸ‡· FranÃ§ais ğŸ¤© | ğŸ‡©ğŸ‡ª Allemand ğŸ¤© | ğŸ‡®ğŸ‡¹ Italien ğŸ¤© | ğŸ‡ªğŸ‡¸ Espagnol ğŸ¤© | ğŸ‡¯ğŸ‡µ Japonais ğŸ˜ | ğŸ‡¨ğŸ‡³ Chinois* ğŸ˜Š

> *Le chinois utilise un modÃ¨le whisper sÃ©parÃ© amÃ©liorÃ© par la ponctuation, pour l'instant...

**La traduction prend en charge toutes les langues, tandis que la langue de doublage dÃ©pend de la mÃ©thode TTS choisie.**

## Installation

Vous rencontrez un problÃ¨me ? Discutez avec notre agent IA gratuit en ligne [**ici**](https://share.fastgpt.in/chat/share?shareId=066w11n3r9aq6879r4z0v9rh) pour vous aider.

> **Note :** Pour les utilisateurs Windows avec un GPU NVIDIA, suivez ces Ã©tapes avant l'installation :
> 1. Installez [CUDA Toolkit 12.6](https://developer.download.nvidia.com/compute/cuda/12.6.0/local_installers/cuda_12.6.0_560.76_windows.exe)
> 2. Installez [CUDNN 9.3.0](https://developer.download.nvidia.com/compute/cudnn/9.3.0/local_installers/cudnn_9.3.0_windows.exe)
> 3. Ajoutez `C:\Program Files\NVIDIA\CUDNN\v9.3\bin\12.6` Ã  votre PATH systÃ¨me
> 4. RedÃ©marrez votre ordinateur

> **Note :** FFmpeg est requis. Veuillez l'installer via les gestionnaires de paquets :
> - Windows : ```choco install ffmpeg``` (via [Chocolatey](https://chocolatey.org/))
> - macOS : ```brew install ffmpeg``` (via [Homebrew](https://brew.sh/))
> - Linux : ```sudo apt install ffmpeg``` (Debian/Ubuntu)

1. Clonez le dÃ©pÃ´t

```bash
git clone https://github.com/Huanshere/VideoLingo.git
cd VideoLingo
```

2. Installez les dÃ©pendances (nÃ©cessite `python=3.10`)

```bash
conda create -n videolingo python=3.10.0 -y
conda activate videolingo
python install.py
```

3. DÃ©marrer l'application

```bash
streamlit run st.py
```

### Docker
Alternativement, vous pouvez utiliser Docker (nÃ©cessite CUDA 12.4 et NVIDIA Driver version >550), voir [Documentation Docker](/docs/pages/docs/docker.en-US.md) :

```bash
docker build -t videolingo .
docker run -d -p 8501:8501 --gpus all videolingo
```

## APIs
VideoLingo prend en charge le format d'API OpenAI et diverses interfaces TTS :
- LLM : `claude-3-5-sonnet`, `gpt-4.1`, `deepseek-v3`, `gemini-2.0-flash`, ... (triÃ©s par performance, soyez prudent avec gemini-2.5-flash...)
- WhisperX : ExÃ©cutez whisperX localement ou utilisez l'API 302.ai
- TTS : `azure-tts`, `openai-tts`, `siliconflow-fishtts`, **`fish-tts`**, `GPT-SoVITS`, `edge-tts`, `*custom-tts`(Vous pouvez modifier votre propre TTS dans custom_tts.py !)

> **Note :** VideoLingo fonctionne avec **[302.ai](https://gpt302.saaslink.net/C2oHR9)** - une seule clÃ© API pour tous les services (LLM, WhisperX, TTS). Ou exÃ©cutez localement avec Ollama et Edge-TTS gratuitement, sans API nÃ©cessaire !

Pour des instructions dÃ©taillÃ©es sur l'installation, la configuration de l'API et le mode batch, veuillez consulter la documentation : [English](/docs/pages/docs/start.en-US.md) | [ä¸­æ–‡](/docs/pages/docs/start.zh-CN.md)

## Limitations actuelles

1. Les performances de transcription de WhisperX peuvent Ãªtre affectÃ©es par le bruit de fond de la vidÃ©o, car il utilise le modÃ¨le wav2vac pour l'alignement. Pour les vidÃ©os avec une musique de fond forte, veuillez activer l'amÃ©lioration de la sÃ©paration vocale. De plus, les sous-titres se terminant par des chiffres ou des caractÃ¨res spÃ©ciaux peuvent Ãªtre tronquÃ©s prÃ©maturÃ©ment en raison de l'incapacitÃ© de wav2vac Ã  mapper les caractÃ¨res numÃ©riques (par exemple, "1") Ã  leur forme parlÃ©e ("un").

2. L'utilisation de modÃ¨les plus faibles peut entraÃ®ner des erreurs lors des processus intermÃ©diaires en raison des exigences strictes de format JSON pour les rÃ©ponses. Si cette erreur se produit, veuillez supprimer le dossier `output` et rÃ©essayer avec un LLM diffÃ©rent, sinon l'exÃ©cution rÃ©pÃ©tÃ©e lira la rÃ©ponse erronÃ©e prÃ©cÃ©dente causant la mÃªme erreur.

3. La fonction de doublage peut ne pas Ãªtre parfaite Ã  100% en raison des diffÃ©rences de dÃ©bit et d'intonation entre les langues, ainsi que de l'impact de l'Ã©tape de traduction. Cependant, ce projet a mis en Å“uvre un traitement d'ingÃ©nierie extensif pour les dÃ©bits de parole afin d'assurer les meilleurs rÃ©sultats de doublage possibles.

4. **La reconnaissance de transcription vidÃ©o multilingue ne conservera que la langue principale**. C'est parce que whisperX utilise un modÃ¨le spÃ©cialisÃ© pour une seule langue lors de l'alignement forcÃ© des sous-titres au niveau des mots, et supprimera les langues non reconnues.

5. **Impossible de doubler sÃ©parÃ©ment plusieurs personnages**, car la capacitÃ© de distinction des locuteurs de whisperX n'est pas suffisamment fiable.

## ğŸ“„ Licence

Ce projet est sous licence Apache 2.0. Remerciements spÃ©ciaux aux projets open source suivants pour leurs contributions :

[whisperX](https://github.com/m-bain/whisperX), [yt-dlp](https://github.com/yt-dlp/yt-dlp), [json_repair](https://github.com/mangiucugna/json_repair), [BELLE](https://github.com/LianjiaTech/BELLE)

## ğŸ“¬ Contactez-moi

- Soumettez des [Issues](https://github.com/Huanshere/VideoLingo/issues) ou des [Pull Requests](https://github.com/Huanshere/VideoLingo/pulls) sur GitHub
- DM moi sur Twitter : [@Huanshere](https://twitter.com/Huanshere)
- Envoyez-moi un email Ã  : team@videolingo.io

## â­ Historique des Ã©toiles

[![Star History Chart](https://api.star-history.com/svg?repos=Huanshere/VideoLingo&type=Timeline)](https://star-history.com/#Huanshere/VideoLingo&Timeline)

---

<p align="center">Si vous trouvez VideoLingo utile, donnez-moi une â­ï¸ !</p> 