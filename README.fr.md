# VideoLingo : Connecter le monde, image par image
<p align="center">
      <a href="https://www.python.org" target="_blank"><img src="https://img.shields.io/badge/Python-3.10-blue.svg" alt="Python"></a>
      <a href="https://github.com/Huanshere/VideoLingo/blob/main/LICENSE" target="_blank"><img src="https://img.shields.io/github/license/Huanshere/VideoLingo.svg" alt="License"></a>
      <a href="https://github.com/Huanshere/VideoLingo/stargazers" target="_blank"><img src="https://img.shields.io/github/stars/Huanshere/VideoLingo.svg" alt="GitHub stars"></a>
    </p>
    
[**ä¸­æ–‡**](README.md) | [**English**](README.en.md)ï½œ[**æ—¥æœ¬èª**](README.ja.md)
    
**Groupe QQ : 875297969**
    
</div>
    
 ## ğŸŒŸ Introduction au projet
    
 VideoLingo est un outil tout-en-un de traduction et de doublage pour vidÃ©os, visant Ã  produire des sous-titres de qualitÃ© Netflix, Ã  Ã©liminer les traductions mÃ©caniques rigides et les sous-titres sur plusieurs lignes, tout en ajoutant un doublage de haute qualitÃ©. Il permet de partager des connaissances au-delÃ  des barriÃ¨res linguistiques Ã  travers le monde. GrÃ¢ce Ã  une interface web intuitive Streamlit, vous pouvez accomplir tout le processus, depuis le lien vidÃ©o jusqu'Ã  l'intÃ©gration de sous-titres bilingues de haute qualitÃ© et mÃªme le doublage, en quelques clics, crÃ©ant facilement des vidÃ©os localisÃ©es de qualitÃ© Netflix.
    
 CaractÃ©ristiques principales et fonctionnalitÃ©s :
    - ğŸ¥ Utilise yt-dlp pour tÃ©lÃ©charger des vidÃ©os depuis des liens YouTube
    
   - ğŸ™ï¸ Utilise WhisperX pour une reconnaissance des sous-titres au niveau des mots
    
- **ğŸ“ Utilise le NLP et GPT pour segmenter les sous-titres en fonction du sens des phrases**
    
- **ğŸ“š GPT rÃ©sume et extrait une base de connaissances terminologiques pour une traduction contextualisÃ©e**
    
- **ğŸ”„ Traduction en trois Ã©tapes (directe, rÃ©flexion et paraphrase), rivalisant avec la qualitÃ© de traduction professionnelle**
    
- **âœ… VÃ©rifie la longueur d'une seule ligne selon les normes Netflix, Ã©vitant strictement les sous-titres en double ligne**
    
- **ğŸ—£ï¸ Utilise des mÃ©thodes comme GPT-SoVITS pour un doublage alignÃ© de haute qualitÃ©**
    
- ğŸš€ Lancement d'un package intÃ©grÃ© en un clic, production de vidÃ©os en un clic via Streamlit
    
- ğŸ“ Journalisation dÃ©taillÃ©e de chaque Ã©tape d'opÃ©ration, avec support de reprise en cas d'interruption
    
- ğŸŒ Support multilingue complet, permettant une localisation vidÃ©o dans plusieurs langues
    
## ğŸ¥ DÃ©monstration
    
  <table>
    <tr>
    <td width="25%">
    
### Traduction russe
---
https://github.com/user-attachments/assets/25264b5b-6931-4d39-948c-5a1e4ce42fa7
    
</td>
    <td width="25%">
    
### GPT-SoVITS
 ---
https://github.com/user-attachments/assets/47d965b2-b4ab-4a0b-9d08-b49a7bf3508c
    
</td>
    <td width="25%">
    
 ### Fish TTS Ding Zhen
 ---
 https://github.com/user-attachments/assets/e7bb9090-d2ef-4261-9dc5-56bd67dc710d
    
 </td>
    <td width="25%">
    
### OAITTS
---
https://github.com/user-attachments/assets/85c64f8c-06cf-4af9-b153-ee9d2897b768
    
   </td>
    </tr>
    </table>
    
### Support linguistique :
    
Langues d'entrÃ©e actuellement supportÃ©es et exemples :
    
| Langue d'entrÃ©e | Niveau de support | DÃ©mo de traduction | DÃ©mo de doublage |
|----------------|-------------------|--------------------|------------------|
| Anglais | ğŸ¤© | [Anglais vers chinois](https://github.com/user-attachments/assets/127373bb-c152-4b7a-8d9d-e586b2c62b4b) | Ã€ venir |
| Russe | ğŸ˜Š | [Russe vers chinois](https://github.com/user-attachments/assets/25264b5b-6931-4d39-948c-5a1e4ce42fa7) | Ã€ venir |
| FranÃ§ais | ğŸ¤© | [FranÃ§ais vers japonais](https://github.com/user-attachments/assets/3ce068c7-9854-4c72-ae77-f2484c7c6630) | Ã€ venir |
| Allemand | ğŸ¤© | [Allemand vers chinois](https://github.com/user-attachments/assets/07cb9d21-069e-4725-871d-c4d9701287a3) | Ã€ venir |
| Italien | ğŸ¤© | [Italien vers chinois](https://github.com/user-attachments/assets/f1f893eb-dad3-4460-aaf6-10cac999195e) | Ã€ venir |
| Espagnol | ğŸ¤© | [Espagnol vers chinois](https://github.com/user-attachments/assets/c1d28f1c-83d2-4f13-a1a1-859bd6cc3553) | Ã€ venir |
| Japonais | ğŸ˜ | [Japonais vers chinois](https://github.com/user-attachments/assets/856c3398-2da3-4e25-9c36-27ca2d1f68c2) | Ã€ venir |
| Chinois* | ğŸ¤© | [Chinois vers anglais](https://github.com/user-attachments/assets/48f746fe-96ff-47fd-bd23-59e9202b495c) | [Talk-show du professeur Luo Xiang](https://github.com/user-attachments/assets/85c64f8c-06cf-4af9-b153-ee9d2897b768) |
    > *Le chinois nÃ©cessite une configuration sÃ©parÃ©e du modÃ¨le whisperX, voir l'installation Ã  partir du code source
    
Le support des langues de traduction couvre toutes les langues gÃ©rÃ©es par le grand modÃ¨le linguistique, tandis que le support des langues pour le doublage dÃ©pend de la mÃ©thode TTS choisie.
    ## ğŸš€ Package intÃ©grÃ© en un clic pour Windows
    
### Notes importantes :
    
1. Le package intÃ©grÃ© utilise la version CPU de torch, d'une taille d'environ **2,6G**.
    2. Lors de l'utilisation de UVR5 pour la sÃ©paration des voix dans l'Ã©tape devoix, la version CPU sera nettement plus lente que torch avec accÃ©lÃ©ration GPU.
3. Le package intÃ©grÃ© **ne supporte que l'appel Ã  whisperXapi â˜ï¸ via API**, et ne supporte pas l'exÃ©cution locale de whisperX ğŸ’».
4. Le whisperXapi utilisÃ© dans le package intÃ©grÃ© ne supporte pas la transcription en chinois. Si vous avez besoin d'utiliser le chinois, veuillez installer Ã  partir du code source et utiliser whisperX localement ğŸ’».
5. Le package intÃ©grÃ© n'a pas encore effectuÃ© la sÃ©paration des voix UVR5 dans l'Ã©tape de transcription, il n'est donc pas recommandÃ© d'utiliser des vidÃ©os avec une musique de fond bruyante.

Si vous avez besoin des fonctionnalitÃ©s suivantes, veuillez installer Ã  partir du code source (nÃ©cessite un GPU Nvidia et au moins **20G** d'espace disque) :
- La langue d'entrÃ©e est le chinois
- ExÃ©cuter whisperX localement ğŸ’»
- Utiliser UVR5 accÃ©lÃ©rÃ© par GPU pour la sÃ©paration des voix
- Transcrire des vidÃ©os avec une musique de fond bruyante

### Instructions de tÃ©lÃ©chargement et d'utilisation

1. TÃ©lÃ©chargez le package en un clic `v1.4` (800 Mo) : [TÃ©lÃ©chargement direct](https://vip.123pan.cn/1817874751/8209290) | [Sauvegarde Baidu](https://pan.baidu.com/s/1H_3PthZ3R3NsjS0vrymimg?pwd=ra64)

2. AprÃ¨s extraction, double-cliquez sur `OneKeyStart.bat` dans le dossier

3. Dans la fenÃªtre du navigateur qui s'ouvre, configurez les paramÃ¨tres nÃ©cessaires dans la barre latÃ©rale, puis crÃ©ez votre vidÃ©o en un clic !
   
  ![attentionen](https://github.com/user-attachments/assets/9ff9d8e1-5422-466f-9e28-1803f23afdc7)

> ğŸ’¡ Remarque : Ce projet nÃ©cessite la configuration de grands modÃ¨les linguistiques, WhisperX et TTS. Veuillez lire attentivement la section **PrÃ©paration de l'API** ci-dessous

## ğŸ“‹ PrÃ©paration de l'API
Ce projet nÃ©cessite l'utilisation de grands modÃ¨les linguistiques, WhisperX et TTS. Plusieurs options sont fournies pour chaque composant. **Veuillez lire attentivement le guide de configuration ğŸ˜Š**
### 1. **Obtenez une API_KEY pour les grands modÃ¨les linguistiques** :

| ModÃ¨le recommandÃ© | Fournisseur recommandÃ© | base_url | Prix | EfficacitÃ© |
|:-----|:---------|:---------|:-----|:---------|
| claude-3-5-sonnet-20240620 (par dÃ©faut) | [Yunwu API](https://yunwu.zeabur.app/register?aff=TXMB) | https://yunwu.zeabur.app | Â¥15 / 1M tokens | ğŸ¤© |
| deepseek-coder | [deepseek](https://platform.deepseek.com/api_keys) | https://api.deepseek.com | Â¥2 / 1M tokens | ğŸ˜² |
> Remarque : L'API Yunwu supporte Ã©galement l'interface tts-1 d'OpenAI, qui peut Ãªtre utilisÃ©e lors de l'Ã©tape de doublage.

> Rappel : deepseek a une trÃ¨s faible probabilitÃ© d'erreurs lors de la traduction. En cas d'erreurs, veuillez passer au modÃ¨le claude 3.5 sonnet.

#### Questions frÃ©quentes

<details>
<summary>Quel modÃ¨le dois-je choisir ?</summary>

- ğŸŒŸ Utilisation par dÃ©faut de Claude 3.5, excellente qualitÃ© de traduction, trÃ¨s bonne cohÃ©rence, sans saveur IA.
- ğŸš€ Si vous utilisez deepseek, la traduction d'une vidÃ©o d'une heure coÃ»te environ Â¥1, avec des rÃ©sultats moyens.
</details>

<details>
<summary>Comment obtenir une clÃ© API ?</summary>

1. Cliquez sur le lien du fournisseur recommandÃ© ci-dessus
2. CrÃ©ez un compte et rechargez-le
3. CrÃ©ez une nouvelle clÃ© API sur la page des clÃ©s API
4. Pour l'API Yunwu, assurez-vous de vÃ©rifier `Quota illimitÃ©`, sÃ©lectionnez le modÃ¨le `claude-3-5-sonnet-20240620`, et il est recommandÃ© de choisir le canal `Pure AZ 1.5x`.
</details>

<details>
<summary>Puis-je utiliser d'autres modÃ¨les ?</summary>

- âœ… Supporte les interfaces d'API similaires Ã  OAI, mais vous devez les changer vous-mÃªme dans la barre latÃ©rale de Streamlit.
- âš ï¸ Cependant, d'autres modÃ¨les (en particulier les petits modÃ¨les) ont une faible capacitÃ© Ã  suivre les instructions et sont trÃ¨s susceptibles de signaler des erreurs lors de la traduction, ce qui est fortement dÃ©conseillÃ©.
</details>

### 2. **PrÃ©parer un token Replicate** (uniquement en utilisant whisperXapi â˜ï¸)

VideoLingo utilise WhisperX pour la reconnaissance vocale, supportant Ã  la fois le dÃ©ploiement local et l'API cloud.
#### Comparaison des options :
| Option | InconvÃ©nients |
|:-----|:-----|
| **whisperX ğŸ–¥ï¸** | â€¢ Installation de CUDA ğŸ› ï¸<br>â€¢ TÃ©lÃ©chargement du modÃ¨le ğŸ“¥<br>â€¢ Exigence de haute VRAM ğŸ’¾ |
| **whisperXapi â˜ï¸** | â€¢ NÃ©cessite un VPN ğŸ•µï¸â€â™‚ï¸<br>â€¢ Carte Visa ğŸ’³<br>â€¢ **Mauvais effet sur le chinois** ğŸš« |

#### Obtenir le token
   - Inscrivez-vous sur [Replicate](https://replicate.com/account/api-tokens), liez un moyen de paiement par carte Visa, et obtenez le token
   - **Ou rejoignez le groupe QQ pour obtenir un token de test gratuit dans l'annonce du groupe**

### 3. **API TTS**
VideoLingo propose plusieurs mÃ©thodes d'intÃ©gration TTS. Voici une comparaison (sautez cette Ã©tape si vous ne faites que traduire sans doublage) :

| Option TTS | Avantages | InconvÃ©nients | Effet en chinois | Effet en langues non-chinoises |
|:---------|:-----|:-----|:---------|:-----------|
| ğŸ™ï¸ OpenAI TTS | Ã‰motions rÃ©alistes | Le chinois sonne comme un Ã©tranger | ğŸ˜• | ğŸ¤© |
| ğŸ”Š Azure TTS  | Effet naturel | Recharge peu pratique | ğŸ¤© | ğŸ˜ƒ |
| ğŸ¤ Fish TTS (recommandÃ©) | Excellent | NÃ©cessite une recharge | ğŸ˜± | ğŸ˜± |
| ğŸ—£ï¸ GPT-SoVITS (bÃªta) | Clonage vocal local | Actuellement supporte uniquement l'entrÃ©e en anglais avec sortie en chinois, nÃ©cessite un GPU pour l'infÃ©rence du modÃ¨le, idÃ©al pour des vidÃ©os mono-personnes sans BGM Ã©vidente, et le modÃ¨le de base doit Ãªtre proche de la voix d'origine | ğŸ˜‚ | ğŸš« |

- Pour OpenAI TTS, nous recommandons d'utiliser [Yunwu API](https://yunwu.zeabur.app/register?aff=TXMB);
- **Les clÃ©s gratuites Azure TTS peuvent Ãªtre obtenues dans l'annonce du groupe QQ** ou vous pouvez vous inscrire et recharger vous-mÃªme sur le [site officiel](https://learn.microsoft.com/zh-cn/azure/ai-services/speech-service/get-started-text-to-speech?tabs=windows%2Cterminal&pivots=programming-language-python);
- **Les clÃ©s gratuites Fish TTS peuvent Ãªtre obtenues dans l'annonce du groupe QQ** ou vous pouvez vous inscrire et recharger vous-mÃªme sur le [site officiel](https://fish.audio/zh-CN/go-api/)

<details>
<summary>Comment choisir une voix OpenAI ?</summary>

Vous pouvez trouver la liste des voix sur le [site officiel](https://platform.openai.com/docs/guides/text-to-speech/voice-options), telles que `alloy`, `echo`, `nova`, et `fable`. Modifiez `OAI_VOICE` dans `config.py` pour changer la voix.

</details>

<details>
<summary>Comment choisir une voix Azure ?</summary>

Il est recommandÃ© d'Ã©couter et de choisir la voix souhaitÃ©e dans l'[expÃ©rience en ligne](https://speech.microsoft.com/portal/voicegallery), et de trouver le code correspondant pour cette voix dans le code Ã  droite, tel que `zh-CN-XiaoxiaoMultilingualNeural`.

</details>

<details>
<summary>Comment choisir une voix Fish TTS ?</summary>

Rendez-vous sur le [site officiel](https://fish.audio/zh-CN/) pour Ã©couter et choisir la voix souhaitÃ©e, et trouvez le code correspondant pour cette voix dans l'URL, comme Ding Zhen est `54a5170264694bfc8e9ad98df7bd89c3`. Les voix populaires ont Ã©tÃ© ajoutÃ©es Ã  `config.py`, modifiez simplement `FISH_TTS_CHARACTER`. Si vous devez utiliser d'autres voix, veuillez modifier le dictionnaire `FISH_TTS_CHARACTER_ID_DICT` dans `config.py`.

</details>

<details>
<summary>Tutoriel d'utilisation de GPT-SoVITS-v2</summary>

1. Consultez le document Yuque [officiel](https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e/dkxgpiy9zb96hob4#KTvnO) pour vÃ©rifier les exigences de configuration et tÃ©lÃ©charger le package intÃ©grÃ©.

2. Placez `GPT-SoVITS-v2-xxx` au mÃªme niveau que le rÃ©pertoire de `VideoLingo`. **Notez qu'ils doivent Ãªtre des dossiers parallÃ¨les.**

3. Choisissez l'une des mÃ©thodes suivantes pour configurer le modÃ¨le :

   a. ModÃ¨le auto-entraÃ®nÃ© :
   - AprÃ¨s l'entraÃ®nement du modÃ¨le, `tts_infer.yaml` sous `GPT-SoVITS-v2-xxx\GPT_SoVITS\configs` sera automatiquement rempli avec l'adresse de votre modÃ¨le. Copiez et renommez-le en `nom_personnage_anglais_voulu.yaml`
   - Dans le mÃªme rÃ©pertoire que le fichier `yaml`, placez l'audio de rÃ©fÃ©rence que vous utiliserez plus tard, nommÃ© `nom_personnage_anglais_voulu_texte_contenu_audio.wav` ou `.mp3`, par exemple `Huanyuv2_Hello, this is a test audio.wav`
   - Dans la barre latÃ©rale de la page web VideoLingo, dÃ©finissez `GPT-SoVITS Character` sur `nom_personnage_anglais_voulu`.

   b. Utilisation du modÃ¨le prÃ©-entraÃ®nÃ© :
   - TÃ©lÃ©chargez mon modÃ¨le depuis [ici](https://vip.123pan.cn/1817874751/8137723), extrayez-le et Ã©crasez-le dans `GPT-SoVITS-v2-xxx`.
   - DÃ©finissez `GPT-SoVITS Character` sur `Huanyuv2`.

   c. Utilisation d'autres modÃ¨les entraÃ®nÃ©s :
   - Placez le fichier de modÃ¨le `xxx.ckpt` dans le dossier `GPT_weights_v2` et le fichier modÃ¨le `xxx.pth` dans le dossier `SoVITS_weights_v2`.
   - RÃ©fÃ©rez-vous Ã  la mÃ©thode a, renommez le fichier `tts_infer.yaml` et modifiez le `t2s_weights_path` et `vits_weights_path` dans la section `custom` du fichier pour pointer vers vos modÃ¨les, par exemple :
  
      ```yaml
      # Exemple de configuration pour la mÃ©thode b :
      t2s_weights_path: GPT_weights_v2/Huanyu_v2-e10.ckpt
      version: v2
      vits_weights_path: SoVITS_weights_v2/Huanyu_v2_e10_s150.pth
      ```
   - RÃ©fÃ©rez-vous Ã  la mÃ©thode a, placez l'audio de rÃ©fÃ©rence que vous utiliserez plus tard dans le mÃªme rÃ©pertoire que le fichier `yaml`, nommÃ© `nom_personnage_anglais_voulu_texte_contenu_audio.wav` ou `.mp3`, par exemple `Huanyuv2_Hello, this is a test audio.wav`. Le programme le reconnaÃ®tra et l'utilisera automatiquement.
   - âš ï¸ Avertissement : **Veuillez utiliser l'anglais pour nommer le `nom_personnage`**, sinon des erreurs se produiront. Le `texte_contenu_audio` peut Ãªtre en chinois. Il est toujours en version bÃªta et peut produire des erreurs.

 ```
   # Structure de rÃ©pertoire attendue :
   .
   â”œâ”€â”€ VideoLingo
   â”‚   â””â”€â”€ ...
   â””â”€â”€ GPT-SoVITS-v2-xxx
       â”œâ”€â”€ GPT_SoVITS
       â”‚   â””â”€â”€ configs
       â”‚       â”œâ”€â”€ tts_infer.yaml
       â”‚       â”œâ”€â”€ nom_personnage_anglais_voulu.yaml
       â”‚       â””â”€â”€ nom_personnage_anglais_voulu_texte_contenu_audio_de_reference.wav
       â”œâ”€â”€ GPT_weights_v2
       â”‚   â””â”€â”€ [Votre fichier modÃ¨le GPT]
       â””â”€â”€ SoVITS_weights_v2
           â””â”€â”€ [Votre fichier modÃ¨le SoVITS]

 ```


     
AprÃ¨s la configuration, assurez-vous de sÃ©lectionner le `Mode Audio de RÃ©fÃ©rence` dans la barre latÃ©rale de la page web. VideoLingo ouvrira automatiquement le port API d'infÃ©rence de GPT-SoVITS dans la ligne de commande pop-up lors de l'Ã©tape de doublage. Vous pouvez le fermer manuellement une fois le doublage terminÃ©. Notez que cette mÃ©thode n'est toujours pas trÃ¨s stable et peut entraÃ®ner des mots ou des phrases manquants, ainsi que d'autres bugs, donc utilisez-la avec prÃ©caution.</details>

## ğŸ› ï¸ Processus d'installation du code source

### PrÃ©requis pour Windows

Avant de commencer l'installation de VideoLingo, assurez-vous d'avoir **20G** d'espace disque libre et complÃ©tez les Ã©tapes suivantes :

| DÃ©pendance | whisperX ğŸ–¥ï¸ | whisperX â˜ï¸ |
|:-----|:-------------------|:----------------|
| Anaconda ğŸ | [TÃ©lÃ©charger](https://www.anaconda.com/products/distribution#download-section) | [TÃ©lÃ©charger](https://www.anaconda.com/products/distribution#download-section) |
| Git ğŸŒ¿ | [TÃ©lÃ©charger](https://git-scm.com/download/win) | [TÃ©lÃ©charger](https://git-scm.com/download/win) |
| Cuda Toolkit 12.6 ğŸš€ | [TÃ©lÃ©charger](https://developer.download.nvidia.com/compute/cuda/12.6.0/local_installers/cuda_12.6.0_560.76_windows.exe) | - |
| Cudnn 9.3.0 ğŸ§  | [TÃ©lÃ©charger](https://developer.download.nvidia.com/compute/cudnn/9.3.0/local_installers/cudnn_9.3.0_windows.exe) | - |

> Remarque : Lors de l'installation d'Anaconda, cochez "Ajouter au chemin systÃ¨me", et redÃ©marrez votre ordinateur aprÃ¨s l'installation ğŸ”„

### Ã‰tapes d'installation

Quelques connaissances en Python sont nÃ©cessaires. Supporte Win, Mac, Linux. Si vous rencontrez des problÃ¨mes, vous pouvez demander Ã  GPT tout au long du processus~

1. Ouvrir l'invite Anaconda et passer au rÃ©pertoire du bureau :
   ```bash
   cd desktop
   ```

2. Cloner le projet et passer au rÃ©pertoire du projet :
   ```bash
   git clone https://github.com/Huanshere/VideoLingo.git
   cd VideoLingo
   ```

3. CrÃ©er et activer l'environnement virtuel (**doit Ãªtre 3.10.0**) :
   ```bash
   conda create -n videolingo python=3.10.0 -y
   conda activate videolingo
   ```

4. ExÃ©cuter le script d'installation :
   ```bash
   python install.py
   ```
   Suivez les instructions pour sÃ©lectionner la mÃ©thode Whisper souhaitÃ©e, le script installera automatiquement les versions correspondantes de torch et whisper.

5. Uniquement pour les utilisateurs qui ont besoin de la transcription en chinois :

   Veuillez tÃ©lÃ©charger manuellement le modÃ¨le Belle-whisper-large-v3-zh-punct ([Lien Baidu](https://pan.baidu.com/s/1NyNtkEM0EMsjdCovncsx0w?pwd=938n)) et le remplacer dans le dossier `_model_cache` Ã  la racine du projet.

6. ğŸ‰ Entrez la commande ou cliquez sur `OneKeyStart.bat` pour lancer l'application Streamlit :
   ```bash
    streamlit run st.py
    ```

7. DÃ©finissez la clÃ© dans la barre latÃ©rale de la page web qui s'ouvre, et assurez-vous de sÃ©lectionner la mÃ©thode whisper.

   ![attentionen](https://github.com/user-attachments/assets/9ff9d8e1-5422-466f-9e28-1803f23afdc7)

8. (Optionnel) Des paramÃ¨tres plus avancÃ©s peuvent Ãªtre modifiÃ©s manuellement dans `config.py`.

<!-- Ce projet utilise un dÃ©veloppement modulaire structurÃ©. Vous pouvez exÃ©cuter les fichiers `core\step__.py` dans l'ordre. Documentation technique : [Chinois](./docs/README_guide_zh.md) | [Anglais](./docs/README_guide_en.md) (Ã€ mettre Ã  jour) -->

## âš ï¸ PrÃ©cautions

1. UVR5 a des exigences Ã©levÃ©es en matiÃ¨re de mÃ©moire. 16 Go de RAM peuvent traiter jusqu'Ã  30 minutes, 32 Go de RAM peuvent traiter jusqu'Ã  50 minutes. Veuillez faire attention aux longues vidÃ©os.
   
2. Il y a une trÃ¨s faible probabilitÃ© d'erreurs de 'phrases' lors de l'Ã©tape de traduction. Si vous en rencontrez, merci de nous en informer.
   
3. La qualitÃ© de la fonction de doublage est instable. Pour une meilleure qualitÃ©, essayez de choisir une vitesse TTS adaptÃ©e Ã  la vidÃ©o originale. Par exemple, la vitesse de OAITTS est relativement rapide, tandis que pour FishTTS, veuillez Ã©couter des Ã©chantillons avant de faire votre choix.

## ğŸ“„ Licence

Ce projet est sous licence Apache 2.0. Lorsque vous utilisez ce projet, veuillez suivre ces rÃ¨gles :

1. Lors de la publication de travaux, il est **recommandÃ© (mais non obligatoire) de crÃ©diter VideoLingo pour la gÃ©nÃ©ration de sous-titres**.
2. Suivez les termes des modÃ¨les de langage large et des TTS utilisÃ©s pour une attribution correcte.
3. Si vous copiez le code, veuillez inclure l'intÃ©gralitÃ© de la licence Apache 2.0.

Nous remercions sincÃ¨rement les projets open-source suivants pour leurs contributions, qui ont fourni un soutien important au dÃ©veloppement de VideoLingo :

- [whisperX](https://github.com/m-bain/whisperX)
- [yt-dlp](https://github.com/yt-dlp/yt-dlp)
- [json_repair](https://github.com/mangiucugna/json_repair)
- [GPT-SoVITS](https://github.com/RVC-Boss/GPT-SoVITS)
- [BELLE](https://github.com/LianjiaTech/BELLE)

## ğŸ“¬ Contactez-nous

- Rejoignez notre groupe QQ : 875297969
- Soumettez [Issues](https://github.com/Huanshere/VideoLingo/issues) ou des [Pull Requests](https://github.com/Huanshere/VideoLingo/pulls) sur GitHub.


## â­ Historique des Stars

[![Star History Chart](https://api.star-history.com/svg?repos=Huanshere/VideoLingo&type=Timeline)](https://star-history.com/#Huanshere/VideoLingo&Timeline)

---

<p align="center">Si vous trouvez VideoLingo utile, n'hÃ©sitez pas Ã  nous donner une â­ï¸!</p>




    
