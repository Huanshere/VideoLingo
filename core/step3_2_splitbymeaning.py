import sys,os,math
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import concurrent.futures
from core.ask_gpt import ask_gpt, step3_2_split_model
from core.prompts_storage import get_split_prompt
from difflib import SequenceMatcher
from config import MAX_SPLIT_LENGTH
import os
import requests
import logging

logger = logging.getLogger(__name__)

def find_split_positions(original, modified):
    split_positions = []
    parts = modified.split('[br]')
    start = 0

    for i in range(len(parts) - 1):  # Only iterate until the second-to-last part
        max_similarity = 0
        best_split = None

        for j in range(start, len(original)):
            if original[j] == ' ':
                original_left = original[start:j]
                modified_left = parts[i].strip()

                left_similarity = SequenceMatcher(None, original_left, modified_left).ratio()

                total_similarity = left_similarity

                if total_similarity > max_similarity:
                    max_similarity = total_similarity
                    best_split = j
        if max_similarity < 0.9:
            print(f"Warning: The best split found has low similarity {max_similarity}")
        if best_split is not None:
            split_positions.append(best_split)
            start = best_split + 1
        else:
            print(f"Warning: Could not find a good split for part {i+1}.")

    return split_positions

def split_sentence(sentence, num_parts, word_limit=18, index=-1, retry_attempt=0):
    """Split a long sentence using GPT and return the result as a string."""
    split_prompt = get_split_prompt(sentence, num_parts, word_limit)
    response_data = ask_gpt(split_prompt + ' ' * retry_attempt, model=step3_2_split_model, response_json=True, log_title='sentence_splitbymeaning')
    best_split_way = response_data[f"split_way_{response_data['best_way']}"]
    split_points = find_split_positions(sentence, best_split_way)
    # split the sentence based on the split points
    for i, split_point in enumerate(split_points):
        if i == 0:
            best_split = sentence[:split_point] + '\n' + sentence[split_point:]
        else:
            parts = best_split.split('\n')
            last_part = parts[-1]
            parts[-1] = last_part[:split_point - split_points[i-1]] + '\n' + last_part[split_point - split_points[i-1]:]
            best_split = '\n'.join(parts)
    if index != -1:
        print(f'âœ… Sentence {index} has been successfully split')
    print(f'ğŸ“„ Original English:   {sentence}')
    print(f"ğŸ“š Split Sentence: {best_split.replace('\n',' [br] ')}")
    
    return best_split

def parallel_split_sentences(sentences, max_length, max_workers, retry_attempt=0):
    """Split sentences in parallel using a thread pool."""
    new_sentences = [None] * len(sentences)
    futures = []
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        for index, sentence in enumerate(sentences):
            tokens = sentence.split()
            num_parts = math.ceil(len(tokens) / max_length)
            if len(tokens) > max_length:
                future = executor.submit(split_sentence, sentence, num_parts, max_length, index=index, retry_attempt=retry_attempt)
                futures.append((future, index, num_parts, sentence))
            else:
                new_sentences[index] = [sentence]

        for future, index, num_parts, sentence in futures:
            split_result = future.result()
            if split_result:
                split_lines = split_result.strip().split('\n')
                new_sentences[index] = [line.strip() for line in split_lines]
            else:
                new_sentences[index] = [sentence]

    return [sentence for sublist in new_sentences for sentence in sublist]

def print_proxy_settings():
    logger.debug("å½“å‰ä»£ç†è®¾ç½®:")
    for var in ['http_proxy', 'https_proxy', 'ftp_proxy', 'no_proxy', 'HTTP_PROXY', 'HTTPS_PROXY', 'FTP_PROXY', 'NO_PROXY']:
        if var in os.environ:
            logger.debug(f"{var}: {os.environ[var]}")
    logger.debug("requests åº“çš„å½“å‰ä»£ç†è®¾ç½®:")
    logger.debug(requests.utils.getproxies())

def split_sentences_by_meaning():
    logger.info("å¼€å§‹æ‰§è¡Œ split_sentences_by_meaning å‡½æ•°")
    print_proxy_settings()
    
    logger.debug("ç¦ç”¨ requests åº“çš„ä»£ç†")
    old_getproxies = requests.utils.getproxies
    requests.utils.getproxies = lambda: {}
    
    old_session = requests.Session
    requests.Session = lambda: type('MockSession', (), {'proxies': {}})()
    
    try:
        logger.debug("å¼€å§‹æ‰§è¡Œä¸»è¦é€»è¾‘")
        # åœ¨è¿™é‡Œæ·»åŠ å‡½æ•°çš„ä¸»è¦é€»è¾‘ï¼Œæ¯ä¸ªé‡è¦æ­¥éª¤éƒ½æ·»åŠ æ—¥å¿—
        # ä¾‹å¦‚ï¼š
        # logger.debug("æ­£åœ¨è¯»å–è¾“å…¥æ–‡ä»¶")
        # with open('input_file.txt', 'r') as f:
        #     content = f.read()
        # logger.debug(f"è¯»å–åˆ°çš„å†…å®¹é•¿åº¦: {len(content)}")
        
        # logger.debug("å¼€å§‹å¤„ç†å†…å®¹")
        # processed_content = process_content(content)
        # logger.debug(f"å¤„ç†åçš„å†…å®¹é•¿åº¦: {len(processed_content)}")
        
        # logger.debug("å¼€å§‹å†™å…¥è¾“å‡ºæ–‡ä»¶")
        # with open('output_file.txt', 'w') as f:
        #     f.write(processed_content)
        # logger.debug("è¾“å‡ºæ–‡ä»¶å†™å…¥å®Œæˆ")
        
    except Exception as e:
        logger.exception("åœ¨ split_sentences_by_meaning å‡½æ•°ä¸­å‘ç”Ÿå¼‚å¸¸")
        raise
    finally:
        logger.debug("æ¢å¤ requests åº“çš„åŸå§‹è®¾ç½®")
        requests.utils.getproxies = old_getproxies
        requests.Session = old_session

    logger.info("split_sentences_by_meaning å‡½æ•°æ‰§è¡Œå®Œæˆ")
    # è¿”å›ç»“æœ
    # ...
    
    try:
        # Read input sentences
        with open('output/log/sentence_splitbymark.txt', 'r', encoding='utf-8') as f:
            sentences = [line.strip() for line in f.readlines()]

        max_length = MAX_SPLIT_LENGTH # 18ä»¥ä¸‹ä¼šåˆ‡å¤ªç¢å½±å“ç¿»è¯‘ï¼Œ22 ä»¥ä¸Šå¤ªé•¿ä¼šå¯¼è‡´åç»­ä¸ºå­—å¹•åˆ‡åˆ†éš¾ä»¥å¯¹é½

        # ğŸ”„ Process sentences multiple times to ensure all are split
        for retry_attempt in range(5):
            sentences = parallel_split_sentences(sentences, max_length=max_length, max_workers=8, retry_attempt=retry_attempt)

        # ğŸ’¾ Save the results
        with open('output/log/sentence_splitbymeaning.txt', 'w', encoding='utf-8') as f:
            f.write('\n'.join(sentences))
        print('âœ… All sentences have been successfully split')

    finally:
        # æ¢å¤ requests åº“çš„åŸå§‹ä»£ç†è®¾ç½®
        requests.utils.getproxies = old_getproxies
        requests.Session = old_session

if __name__ == '__main__':
    print(split_sentence('Which makes no sense to the... average guy who always pushes the character creation slider all the way to the right.', 2, 22))
    # split_sentences_by_meaning()
