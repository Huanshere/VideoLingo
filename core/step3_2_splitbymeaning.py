import sys,os,math
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import concurrent.futures
from core.ask_gpt import ask_gpt
from core.prompts_storage import get_split_prompt
from difflib import SequenceMatcher
import math
from core.spacy_utils.load_nlp_model import init_nlp
from config import get_joiner, WHISPER_LANGUAGE
from core.step2_whisper_stamped import get_whisper_language

def tokenize_sentence(sentence, nlp):
    # åˆ†è¯å™¨ ç»Ÿè®¡å¥å­å•è¯æ•°é‡
    doc = nlp(sentence)
    return [token.text for token in doc]

def find_split_positions(original, modified):
    split_positions = []
    parts = modified.split('[br]')
    start = 0
    language = get_whisper_language() if WHISPER_LANGUAGE == 'auto' else WHISPER_LANGUAGE
    joiner = get_joiner(language)

    for i in range(len(parts) - 1):
        max_similarity = 0
        best_split = None

        for j in range(start, len(original)):
            original_left = original[start:j]
            modified_left = joiner.join(parts[i].split())

            left_similarity = SequenceMatcher(None, original_left, modified_left).ratio()

            if left_similarity > max_similarity:
                max_similarity = left_similarity
                best_split = j

        if max_similarity < 0.9:
            print(f"è­¦å‘Šï¼šæ‰¾åˆ°çš„æœ€ä½³åˆ†å‰²ç‚¹ç›¸ä¼¼åº¦è¾ƒä½ {max_similarity}")
        if best_split is not None:
            split_positions.append(best_split)
            start = best_split
        else:
            print(f"è­¦å‘Šï¼šæ— æ³•ä¸ºç¬¬ {i+1} éƒ¨åˆ†æ‰¾åˆ°åˆé€‚çš„åˆ†å‰²ç‚¹ã€‚")

    return split_positions

def split_sentence(sentence, num_parts, word_limit=18, index=-1, retry_attempt=0):
    """Split a long sentence using GPT and return the result as a string."""
    split_prompt = get_split_prompt(sentence, num_parts, word_limit)
    from config import step3_2_split_model
    response_data = ask_gpt(split_prompt + ' ' * retry_attempt, model=step3_2_split_model, response_json=True, log_title='sentence_splitbymeaning')
    best_split_way = response_data[f"split_way_{response_data['best_way']}"]
    split_points = find_split_positions(sentence, best_split_way)
    # split the sentence based on the split points
    for i, split_point in enumerate(split_points):
        if i == 0:
            best_split = sentence[:split_point] + '\n' + sentence[split_point:]
        else:
            parts = best_split.split('\n')
            last_part = parts[-1]
            parts[-1] = last_part[:split_point - split_points[i-1]] + '\n' + last_part[split_point - split_points[i-1]:]
            best_split = '\n'.join(parts)
    if index != -1:
        print(f'âœ… Sentence {index} has been successfully split')
    print("best_split:",best_split)
    print(f'ğŸ“„ Original English:   {sentence}')
    print_split = best_split.replace('\n',' [br] ')
    print(f"ğŸ“š Split Sentence: {print_split}")
    
    return best_split

def parallel_split_sentences(sentences, max_length, max_workers, nlp, retry_attempt=0):
    """Split sentences in parallel using a thread pool."""
    new_sentences = [None] * len(sentences)
    futures = []

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        for index, sentence in enumerate(sentences):
            # æŒ‰ç…§ç©ºæ ¼åˆ‡å‰²
            # tokens = sentence.split() # TODO ä½¿ç”¨åˆ†è¯å™¨
            tokens = tokenize_sentence(sentence, nlp)
            print("åˆ†è¯ç»“æœï¼š",tokens)
            num_parts = math.ceil(len(tokens) / max_length)
            if len(tokens) > max_length:
                future = executor.submit(split_sentence, sentence, num_parts, max_length, index=index, retry_attempt=retry_attempt)
                futures.append((future, index, num_parts, sentence))
            else:
                new_sentences[index] = [sentence]

        for future, index, num_parts, sentence in futures:
            split_result = future.result()
            if split_result:
                split_lines = split_result.strip().split('\n')
                new_sentences[index] = [line.strip() for line in split_lines]
            else:
                new_sentences[index] = [sentence]

    return [sentence for sublist in new_sentences for sentence in sublist]

def split_sentences_by_meaning():
    """æŒ‰æ„ä¹‰åˆ†å‰²å¥å­çš„ä¸»è¦å‡½æ•°ã€‚"""
    # è¯»å–è¾“å…¥çš„å¥å­
    with open('output/log/sentence_splitbynlp.txt', 'r', encoding='utf-8') as f:
        sentences = [line.strip() for line in f.readlines()]

    nlp = init_nlp()
    # ğŸ”„ å¤šæ¬¡å¤„ç†å¥å­ä»¥ç¡®ä¿å…¨éƒ¨è¢«åˆ†å‰²
    from config import MAX_WORKERS, MAX_SPLIT_LENGTH
    for retry_attempt in range(5):
        sentences = parallel_split_sentences(sentences, max_length=MAX_SPLIT_LENGTH, max_workers=MAX_WORKERS, nlp=nlp, retry_attempt=retry_attempt)

    # ğŸ’¾ ä¿å­˜ç»“æœ
    with open('output/log/sentence_splitbymeaning.txt', 'w', encoding='utf-8') as f:
        f.write('\n'.join(sentences))
    print('âœ… æ‰€æœ‰å¥å­å·²æˆåŠŸåˆ†å‰²')

if __name__ == '__main__':
    # print(split_sentence('Which makes no sense to the... average guy who always pushes the character creation slider all the way to the right.', 2, 22))
    split_sentences_by_meaning()