import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
import os,sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from core.spacy_utils.load_nlp_model import init_nlp
from config import get_joiner, WHISPER_LANGUAGE
from core.step2_whisper import get_whisper_language

def split_long_sentence(doc):
    tokens = [token.text for token in doc]
    n = len(tokens)
    
    # åŠ¨æ€è§„åˆ’æ•°ç»„ï¼Œdp[i]è¡¨ç¤ºä»å¼€å§‹åˆ°ç¬¬iä¸ªtokençš„æœ€ä¼˜åˆ†å‰²æ–¹æ¡ˆ
    dp = [float('inf')] * (n + 1)
    dp[0] = 0
    
    # è®°å½•æœ€ä¼˜åˆ†å‰²ç‚¹
    prev = [0] * (n + 1)
    
    for i in range(1, n + 1):
        for j in range(max(0, i - 100), i):  # é™åˆ¶æŸ¥æ‰¾èŒƒå›´ï¼Œé¿å…è¿‡é•¿çš„å¥å­
            if i - j >= 30:  # ç¡®ä¿åˆ†å¥é•¿åº¦è‡³å°‘ä¸º30
                token = doc[i-1]
                if j == 0 or (token.is_sent_end or token.pos_ in ['VERB', 'AUX'] or token.dep_ == 'ROOT'):
                    if dp[j] + 1 < dp[i]:
                        dp[i] = dp[j] + 1
                        prev[i] = j
    
    # æ ¹æ®æœ€ä¼˜åˆ†å‰²ç‚¹é‡å»ºå¥å­
    sentences = []
    i = n
    language = get_whisper_language() if WHISPER_LANGUAGE == 'auto' else WHISPER_LANGUAGE # è€ƒè™‘å¼ºåˆ¶è‹±æ–‡çš„æƒ…å†µ
    joiner = get_joiner(language)
    while i > 0:
        j = prev[i]
        sentences.append(joiner.join(tokens[j:i]).strip())
        i = j
    
    return sentences[::-1]  # åè½¬åˆ—è¡¨ä»¥ä¿æŒåŸå§‹é¡ºåº

def split_extremely_long_sentence(doc):
    tokens = [token.text for token in doc]
    n = len(tokens)
    
    num_parts = (n + 59) // 60  # å‘ä¸Šå–æ•´
    
    part_length = n // num_parts
    
    sentences = []
    language = get_whisper_language() if WHISPER_LANGUAGE == 'auto' else WHISPER_LANGUAGE # è€ƒè™‘å¼ºåˆ¶è‹±æ–‡çš„æƒ…å†µ
    joiner = get_joiner(language)
    for i in range(num_parts):
        start = i * part_length
        end = start + part_length if i < num_parts - 1 else n
        sentence = joiner.join(tokens[start:end])
        sentences.append(sentence)
    
    return sentences

def split_long_by_root_main(nlp):

    with open("output/log/sentence_splitbyconnector.txt", "r", encoding="utf-8") as input_file:
        sentences = input_file.readlines()

    all_split_sentences = []
    for sentence in sentences:
        doc = nlp(sentence.strip())
        if len(doc) > 60:
            split_sentences = split_long_sentence(doc)
            if any(len(nlp(sent)) > 60 for sent in split_sentences):
                split_sentences = [subsent for sent in split_sentences for subsent in split_extremely_long_sentence(nlp(sent))]
            all_split_sentences.extend(split_sentences)
            print(f"âœ‚ï¸  åˆ†å‰²é•¿å¥: {sentence[:30]}...")
        else:
            all_split_sentences.append(sentence.strip())

    with open("output/log/sentence_splitbynlp.txt", "w", encoding="utf-8") as output_file:
        for sentence in all_split_sentences:
            output_file.write(sentence + "\n")

    print("ğŸ’¾ Long sentences split by root saved to â†’  `sentence_splitbynlp.txt`")

if __name__ == "__main__":
    nlp = init_nlp()
    split_long_by_root_main(nlp)
    # raw = "å¹³å£ã•ã‚“ã®ç››ã‚Šä¸Šã’ã”ã¾ãŒåˆã‚ã¦å£²ã‚Œã¾ã—ãŸæœ¬å½“ã«å¬‰ã—ã„ã§ã™æœ¬å½“ã«ã‚„ã£ã±ã‚Šè¦‹ãŸç¬é–“ã„ã„ã£ã¦è¨€ã£ã¦ãã‚Œã‚‹ãã†ã„ã†ã‚³ãƒã‚’ä½œã‚‹ã®ãŒã‚„ã£ã±ã‚Šã„ã„ã§ã™ã‚ˆã­ãã®2ãƒ¶æœˆå¾Œãƒã‚³ã•ã‚“ãŒä½•ã‚„ã‚‰ãã‚ãã‚ã—ã¦ã„ã¾ã—ãŸãªã‚“ã‹æ°—æŒã¡æ‚ªã„ã‚„ã£ã¦ããŸã®ã¯å¹³å£ã•ã‚“ã®é§’ã®è©•åˆ¤ã‚’èãã¤ã‘ãŸæ„›çŸ¥çœŒã®åé›†å®¶ã§ã™ã“ã®ç”·æ€§å¸«åŒ å¤§æ²¢ã•ã‚“ã®é§’ã‚‚æŒã£ã¦ã„ã‚‹ã¨ã„ã„ã¾ã™ã¡ã‚‡ã£ã¨è¤’ã‚ã™ãã‹ãªã§ã‚‚ç¢ºå®Ÿã«ãƒ•ã‚¡ãƒ³ã¯åºƒãŒã£ã¦ã„ã‚‹ã‚ˆã†ã§ã™è‡ªä¿¡ãŒãªã„éƒ¨åˆ†ã‚’ã™ã”ãæ„Ÿã˜ã¦ãŸã®ã§ã“ã‚Œã§è‡ªä¿¡ã‚’æŒã£ã¦é€²ã‚“ã§ãã‚Œã‚‹ãªã£ã¦ã„ã†æœ¬å½“ã«å§‹ã¾ã£ãŸã°ã£ã‹ã‚Šã“ã‚Œã‹ã‚‰ã„ã‚ã„ã‚æŒ‘æˆ¦ã—ã¦ã„ã£ã¦ãã‚Œã‚‹ã¨ã„ã„ãªã¨æ€ã£ã¦ä»Šæœˆå¹³å£ã•ã‚“ã¯ã‚ã‚‹å ´æ‰€ã‚’è¨ªã‚Œã¾ã—ãŸã“ã‚Œã¾ã§æ•°ã€…ã®ã‚¿ã‚¤ãƒˆãƒ«æˆ¦ã§ã‚³ãƒã‚’æä¾›ã—ã¦ããŸè€èˆ—5ç•ªæ‰‹å¹³å£ã•ã‚“ã®ã‚³ãƒã‚’æ‰±ã„ãŸã„ã¨è¨€ã„ã¾ã™ã„ã„ã§ã™ã­ã‡å›°ã£ã¦ã ã‚“ã ã‚“æˆé•·ã—ã¾ã™ã®ã§å¤§åˆ‡ã«ä½¿ã£ã¦ãã†ã„ã†é•·ãè‰¯ã„é§’ã«ãªã‚‹é§’ã§ã™ã­å•†è«‡ãŒçµ‚ã‚ã£ãŸå¾Œåº—ä¸»ãŒã‚ã‚‹ã‚‚ã®ã‚’å–ã‚Šå‡ºã—ã¾ã—ãŸã“ã®å‰ã®åäººæˆ¦ã§ä½¿ã£ãŸé§’ãŒã‚ã‚‹ã‚“ã§ã™ã‘ã©å»å¹´ã€åäººéŠ­ã§ä½¿ã‚ã‚ŒãŸç››ã‚Šä¸Šã’ã”ã¾ä½ãç››ã‚Šä¸Šã’ã¦å“è‰¯ãã™ã‚‹ã¨ã„ã†ã®ã¯é›£ã—ã„ç´ æ™´ã‚‰ã—ã„ã§ã™ã­å¹³å£ã•ã‚“ãŒç›®æŒ‡ã™é«˜ã¿ã§ã™ã“ã†ã„ã£ãŸæ„Ÿã˜ã§ä½œã‚Œã‚Œã°ã¾ã ã¾ã ã§ã™ã‘ã©ãŸã ã€å¤šåˆ†ã€å’²ãã€‚"
    # nlp = init_nlp()
    # doc = nlp(raw.strip())
    # for sent in split_still_long_sentence(doc):
    #     print(sent, '\n==========')
