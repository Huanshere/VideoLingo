import os
import string
import warnings
from core.spacy_utils.load_nlp_model import init_nlp, SPLIT_BY_CONNECTOR_FILE
from core.utils import *
from core.utils.models import _3_1_SPLIT_BY_NLP

warnings.filterwarnings("ignore", category=FutureWarning)

def split_long_sentence(doc):
    tokens = [token.text for token in doc]
    n = len(tokens)
    
    # dynamic programming array, dp[i] represents the optimal split scheme from the start to the ith token
    dp = [float('inf')] * (n + 1)
    dp[0] = 0
    
    # record optimal split points
    prev = [0] * (n + 1)
    
    for i in range(1, n + 1):
        for j in range(max(0, i - 100), i):  # limit search range to avoid overly long sentences
            if i - j >= 30:  # ensure sentence length is at least 30
                token = doc[i-1]
                if j == 0 or (token.is_sent_end or token.pos_ in ['VERB', 'AUX'] or token.dep_ == 'ROOT'):
                    if dp[j] + 1 < dp[i]:
                        dp[i] = dp[j] + 1
                        prev[i] = j
    
    # rebuild sentences based on optimal split points
    sentences = []
    i = n
    whisper_language = load_key("whisper.language")
    language = load_key("whisper.detected_language") if whisper_language == 'auto' else whisper_language # consider force english case
    joiner = get_joiner(language)
    while i > 0:
        j = prev[i]
        sentences.append(joiner.join(tokens[j:i]).strip())
        i = j
    
    return sentences[::-1]  # reverse list to keep original order

def split_extremely_long_sentence(doc):
    tokens = [token.text for token in doc]
    n = len(tokens)
    
    num_parts = (n + 59) // 60  # round up
    
    part_length = n // num_parts
    
    sentences = []
    whisper_language = load_key("whisper.language")
    language = load_key("whisper.detected_language") if whisper_language == 'auto' else whisper_language # consider force english case
    joiner = get_joiner(language)
    for i in range(num_parts):
        start = i * part_length
        end = start + part_length if i < num_parts - 1 else n
        sentence = joiner.join(tokens[start:end])
        sentences.append(sentence)
    
    return sentences


def split_long_by_root_main(nlp):
    with open(SPLIT_BY_CONNECTOR_FILE, "r", encoding="utf-8") as input_file:
        sentences = input_file.readlines()

    all_split_sentences = []
    for sentence in sentences:
        doc = nlp(sentence.strip())
        if len(doc) > 60:
            split_sentences = split_long_sentence(doc)
            if any(len(nlp(sent)) > 60 for sent in split_sentences):
                split_sentences = [subsent for sent in split_sentences for subsent in split_extremely_long_sentence(nlp(sent))]
            all_split_sentences.extend(split_sentences)
            rprint(f"[yellow]âœ‚ï¸  Splitting long sentences by root: {sentence[:30]}...[/yellow]")
        else:
            all_split_sentences.append(sentence.strip())

    punctuation = string.punctuation + "'" + '"'  # include all punctuation and apostrophe ' and "

    with open(_3_1_SPLIT_BY_NLP, "w", encoding="utf-8") as output_file:
        for i, sentence in enumerate(all_split_sentences):
            stripped_sentence = sentence.strip()
            if not stripped_sentence or all(char in punctuation for char in stripped_sentence):
                rprint(f"[yellow]âš ï¸  Warning: Empty or punctuation-only line detected at index {i}[/yellow]")
                if i > 0:
                    all_split_sentences[i-1] += sentence
                continue
            output_file.write(sentence + "\n")

    # delete the original file
    os.remove(SPLIT_BY_CONNECTOR_FILE)   

    rprint(f"[green]ğŸ’¾ Long sentences split by root saved to â†’  {_3_1_SPLIT_BY_NLP}[/green]")

if __name__ == "__main__":
    nlp = init_nlp()
    split_long_by_root_main(nlp)
    # raw = "å¹³å£ã•ã‚“ã®ç››ã‚Šä¸Šã’ã”ã¾ãŒåˆã‚ã¦å£²ã‚Œã¾ã—ãŸæœ¬å½“ã«å¬‰ã—ã„ã§ã™æœ¬å½“ã«ã‚„ã£ã±ã‚Šè¦‹ãŸç¬é–“ã„ã„ã£ã¦è¨€ã£ã¦ãã‚Œã‚‹ãã†ã„ã†ã‚³ãƒã‚’ä½œã‚‹ã®ãŒã‚„ã£ã±ã‚Šã„ã„ã§ã™ã‚ˆã­ãã®2ãƒ¶æœˆå¾Œãƒã‚³ã•ã‚“ãŒä½•ã‚„ã‚‰ãã‚ãã‚ã—ã¦ã„ã¾ã—ãŸãªã‚“ã‹æ°—æŒã¡æ‚ªã„ã‚„ã£ã¦ããŸã®ã¯å¹³å£ã•ã‚“ã®é§’ã®è©•åˆ¤ã‚’èãã¤ã‘ãŸæ„›çŸ¥çœŒã®åé›†å®¶ã§ã™ã“ã®ç”·æ€§å¸«åŒ å¤§æ²¢ã•ã‚“ã®é§’ã‚‚æŒã£ã¦ã„ã‚‹ã¨ã„ã„ã¾ã™ã¡ã‚‡ã£ã¨è¤’ã‚ã™ãã‹ãªã§ã‚‚ç¢ºå®Ÿã«ãƒ•ã‚¡ãƒ³ã¯åºƒãŒã£ã¦ã„ã‚‹ã‚ˆã†ã§ã™è‡ªä¿¡ãŒãªã„éƒ¨åˆ†ã‚’ã™ã”ãæ„Ÿã˜ã¦ãŸã®ã§ã“ã‚Œã§è‡ªä¿¡ã‚’æŒã£ã¦é€²ã‚“ã§ãã‚Œã‚‹ãªã£ã¦ã„ã†æœ¬å½“ã«å§‹ã¾ã£ãŸã°ã£ã‹ã‚Šã“ã‚Œã‹ã‚‰ã„ã‚ã„ã‚æŒ‘æˆ¦ã—ã¦ã„ã£ã¦ãã‚Œã‚‹ã¨ã„ã„ãªã¨æ€ã£ã¦ä»Šæœˆå¹³å£ã•ã‚“ã¯ã‚ã‚‹å ´æ‰€ã‚’è¨ªã‚Œã¾ã—ãŸã“ã‚Œã¾ã§æ•°ã€…ã®ã‚¿ã‚¤ãƒˆãƒ«æˆ¦ã§ã‚³ãƒã‚’æä¾›ã—ã¦ããŸè€èˆ—5ç•ªæ‰‹å¹³å£ã•ã‚“ã®ã‚³ãƒã‚’æ‰±ã„ãŸã„ã¨è¨€ã„ã¾ã™ã„ã„ã§ã™ã­ã‡å›°ã£ã¦ã ã‚“ã ã‚“æˆé•·ã—ã¾ã™ã®ã§å¤§åˆ‡ã«ä½¿ã£ã¦ãã†ã„ã†é•·ãè‰¯ã„é§’ã«ãªã‚‹é§’ã§ã™ã­å•†è«‡ãŒçµ‚ã‚ã£ãŸå¾Œåº—ä¸»ãŒã‚ã‚‹ã‚‚ã®ã‚’å–ã‚Šå‡ºã—ã¾ã—ãŸã“ã®å‰ã®åäººæˆ¦ã§ä½¿ã£ãŸé§’ãŒã‚ã‚‹ã‚“ã§ã™ã‘ã©å»å¹´ã€åäººéŠ­ã§ä½¿ã‚ã‚ŒãŸç››ã‚Šä¸Šã’ã”ã¾ä½ãç››ã‚Šä¸Šã’ã¦å“è‰¯ãã™ã‚‹ã¨ã„ã†ã®ã¯é›£ã—ã„ç´ æ™´ã‚‰ã—ã„ã§ã™ã­å¹³å£ã•ã‚“ãŒç›®æŒ‡ã™é«˜ã¿ã§ã™ã“ã†ã„ã£ãŸæ„Ÿã˜ã§ä½œã‚Œã‚Œã°ã¾ã ã¾ã ã§ã™ã‘ã©ãŸã ã€å¤šåˆ†ã€å’²ãã€‚"
    # nlp = init_nlp()
    # doc = nlp(raw.strip())
    # for sent in split_still_long_sentence(doc):
    #     print(sent, '\n==========')
