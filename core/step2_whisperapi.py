import os,sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import subprocess
from openai import OpenAI
import pandas as pd
from typing import List, Dict
from uvr5.uvr5_for_videolingo import uvr5_for_videolingo
import librosa
import numpy as np
import json

def convert_video_to_audio(input_file: str):
    # ğŸ¬â¡ï¸ğŸµ Convert video to audio
    # audio_file = os.path.splitext(input_file)[0] + '_temp.mp3'
    os.makedirs('output/audio', exist_ok=True)
    audio_file = 'output/audio/raw_full_audio.wav'

    if not os.path.exists(audio_file):
        # Convert video to audio using single line ffmpeg command
        ffmpeg_cmd = [
                'ffmpeg',
                '-i', input_file,
                '-vn',
                '-acodec', 'libmp3lame',
                '-ar', '16000',
                '-b:a', '64k',
                audio_file
            ]
        print(f"ğŸ¬â¡ï¸ğŸµ æ­£åœ¨è½¬æ¢ä¸ºéŸ³é¢‘......")
        subprocess.run(ffmpeg_cmd, check=True, stderr=subprocess.PIPE)
        print(f"ğŸ¬â¡ï¸ğŸµ å·²å°† <{input_file}> è½¬æ¢ä¸º <{audio_file}>\n")

    # Check file size
    if os.path.getsize(audio_file) > 25 * 1024 * 1024:
        print("âš ï¸ æ–‡ä»¶å¤§å°è¶…è¿‡25MBã€‚è¯·ä½¿ç”¨æ›´å°çš„æ–‡ä»¶ã€‚")
        return None

    return audio_file

def detect_background_music(audio_file: str, threshold: float = 20.0) -> bool:
    print(f"ğŸµâ¡ï¸ğŸ” æ­£åœ¨æ£€æµ‹èƒŒæ™¯éŸ³ä¹...")
    y, sr = librosa.load(audio_file)
    S = np.abs(librosa.stft(y))
    contrast = librosa.feature.spectral_contrast(S=S, sr=sr)
    mean_contrast = np.mean(contrast)
    
    print(f"å¹³å‡é¢‘è°±å¯¹æ¯”åº¦: {mean_contrast}")
    return mean_contrast > threshold

def uvr5_process(audio_file: str):
    audio_dir = os.path.dirname(audio_file)
    audio_name = os.path.basename(audio_file)
    vocal_file = os.path.join(audio_dir, 'raw_vocal_uvr.wav')
    bg_file = os.path.join(audio_dir, 'raw_background_uvr.wav')
    comp_vocal = os.path.join(audio_dir, 'raw_vocal.mp3')
    comp_bg = os.path.join(audio_dir, 'raw_background.mp3')

    if not os.path.exists(comp_vocal) or not os.path.exists(comp_bg):
        if not os.path.exists(vocal_file) and detect_background_music(audio_file):
            print("ğŸµâ¡ï¸ğŸµ æ­£åœ¨ä½¿ç”¨uvr5åˆ†ç¦»äººå£°å’Œä¼´å¥......")
            uvr5_for_videolingo(audio_file, save_dir=audio_dir)
            os.rename(os.path.join(audio_dir, f'vocal_{audio_name}_10.wav'), vocal_file)
            os.rename(os.path.join(audio_dir, f'instrument_{audio_name}_10.wav'), bg_file)
        else:
            print("æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„èƒŒæ™¯éŸ³ä¹æˆ–å·²å¤„ç†ï¼Œè·³è¿‡UVRå¤„ç†ã€‚")
            return audio_file

        for in_file, out_file, type_name in [
            (vocal_file, comp_vocal, "äººå£°"),
            (bg_file, comp_bg, "èƒŒæ™¯")
        ]:
            print(f"ğŸµâ¡ï¸ğŸ—œï¸ æ­£åœ¨å‹ç¼©{type_name}éŸ³é¢‘æ–‡ä»¶......")
            subprocess.run([
                'ffmpeg',
                '-i', in_file,
                '-ar', '16000',
                '-b:a', '64k',
                out_file
            ], check=True, stderr=subprocess.PIPE)
            print(f"ğŸµâ¡ï¸ğŸ—œï¸ {type_name}éŸ³é¢‘æ–‡ä»¶å·²å‹ç¼©: {out_file}")
            # åˆ é™¤åŸå§‹æ–‡ä»¶
            os.remove(in_file)
    else:
        print("ğŸµâ¡ï¸ğŸµ UVRå¤„ç†å’Œå‹ç¼©å·²å®Œæˆï¼Œè·³è¿‡å¤„ç†ã€‚")

    return comp_vocal

def transcribe_audio(audio_file: str):
    from config import WHISPER_API_KEY, BASE_URL
    print(f"ğŸµâ¡ï¸ğŸ“ æ­£åœ¨è½¬å½•éŸ³é¢‘{audio_file}ä¸ºæ–‡æœ¬......")
    client = OpenAI(
        base_url=BASE_URL+"/v1",
        api_key=WHISPER_API_KEY
    )

    audio = open(audio_file, "rb")
    transcript = client.audio.transcriptions.create(
        file=audio,
        model="whisper-1",
        response_format="verbose_json",
        timestamp_granularities=["word"],
    )

    # ä¿å­˜åŸå§‹è½¬å½•æ–‡æœ¬
    os.makedirs('output/log', exist_ok=True)
    with open("output/log/raw_transcript.txt", "w", encoding='utf-8') as f:
        f.write(transcript.text)

    print(f"ğŸµâ¡ï¸ğŸ“ è½¬å½•éŸ³é¢‘ä¸ºæ–‡æœ¬å®Œæˆï¼Œè¯†åˆ«è¯­è¨€ä¸º: {transcript.language}")
    with open("output/log/transcript_language.json", "w", encoding='utf-8') as f:
        json.dump({"language": transcript.language}, f)

    # å¤„ç†è½¬å½•ç»“æœ
    all_words: List[Dict[str, float]] = [
        {'text': f'"{word_info["word"]}"', 'start': round(word_info['start'], 2), 'end': round(word_info['end'], 2)}
        for word_info in transcript.words
    ]

    df = pd.DataFrame(all_words)

    # ğŸ’¾ å°†è½¬å½•ç»“æœä¿å­˜ä¸ºExcelæ–‡ä»¶
    excel_path = os.path.join('output/log', "cleaned_chunks.xlsx")
    df.to_excel(excel_path, index=False)
    print(f"ğŸ“Š Excelæ–‡ä»¶å·²ä¿å­˜åˆ° {excel_path}")

    return df

def get_whisper_language():
    try:
        with open("output/log/transcript_language.json", "r", encoding='utf-8') as f:
            language = json.load(f)["language"]
        return language
    except:
        print("æ— æ³•è¯»å–è¯­è¨€ä¿¡æ¯")
        return None

def transcribe(video_file: str):
    if not os.path.exists("output/log/cleaned_chunks.xlsx"):
        # ğŸ¥â¡ï¸ğŸµ å°†è§†é¢‘è½¬æ¢ä¸ºéŸ³é¢‘
        audio_file = convert_video_to_audio(video_file)
        if audio_file:
            #! æš‚æ—¶ä¿ç•™ï¼Œ uvr5 æ•ˆæœä¸€æ¬¡ä¸å¤Ÿæ„Ÿè§‰
            # vocal_file = uvr5_process(audio_file)
            # ğŸµâ¡ï¸ğŸ“ è½¬å½•éŸ³é¢‘ä¸ºæ–‡æœ¬å¹¶ä¿å­˜ç»“æœ
            # transcribe_audio(vocal_file)
            transcribe_audio(audio_file)
    else:
        print("ğŸ“Š è½¬å½•ç»“æœå·²å­˜åœ¨,è·³è¿‡è½¬å½•æ­¥éª¤ã€‚")

if __name__ == "__main__":
    from core.step1_ytdlp import find_video_files
    video_file = find_video_files()
    transcribe(video_file)