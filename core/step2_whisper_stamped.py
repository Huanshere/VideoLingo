import os,sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import subprocess
import whisper_timestamped as whisper
import torch
import pandas as pd
from typing import List, Dict
import warnings
warnings.filterwarnings("ignore")
import json

def convert_video_to_audio_and_transcribe(input_file: str):
    from config import WHISPER_MODEL, MODEL_DIR, WHISPER_LANGUAGE
    # ğŸ¬â¡ï¸ğŸµâ¡ï¸ğŸ“Š Convert video to audio and transcribe
    # audio_file = os.path.splitext(input_file)[0] + '_temp.mp3'
    os.makedirs('output/audio', exist_ok=True)
    audio_file = 'output/audio/raw_full_audio.wav'
    
    if not os.path.exists(audio_file):
        # Convert video to audio
        ffmpeg_cmd = [
            'ffmpeg',
            '-i', input_file,
            '-vn',
            '-acodec', 'libmp3lame',
            '-ar', '16000',
            '-b:a', '64k',
            audio_file
        ]
        print(f"ğŸ¬â¡ï¸ğŸµ æ­£åœ¨è½¬æ¢ä¸ºéŸ³é¢‘......")
        subprocess.run(ffmpeg_cmd, check=True, stderr=subprocess.PIPE)
        print(f"ğŸ¬â¡ï¸ğŸµ å·²å°† <{input_file}> è½¬æ¢ä¸º <{audio_file}>\n")
    
    # Check file size
    if os.path.getsize(audio_file) > 25 * 1024 * 1024:
        print("âš ï¸ æ–‡ä»¶å¤§å°è¶…è¿‡25MBã€‚è¯·ä½¿ç”¨æ›´å°çš„æ–‡ä»¶ã€‚")
        return None
    
    # Transcribe audio
    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸš€ æ­£åœ¨å¯åŠ¨Whisper...\nğŸ–¥ï¸  ASRè®¾å¤‡: {device}")
    print("æ­¤æ­¥éª¤ä¼šèŠ±è´¹å¾ˆé•¿æ—¶é—´ï¼Œå°¤å…¶ä¼šåœ¨100%åä»ç„¶å¤„ç†å¾ˆé•¿æ—¶é—´...")
    
    audio = whisper.load_audio(audio_file)
    os.makedirs(MODEL_DIR, exist_ok=True)
    model = whisper.load_model(WHISPER_MODEL, device=device, download_root=MODEL_DIR)
    if WHISPER_LANGUAGE == 'auto':
        # result = whisper.transcribe(model, audio, beam_size=5, best_of=5, detect_disfluencies=True, vad=True, temperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0))
        result = whisper.transcribe(model, audio, beam_size=5, best_of=5, temperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0))
    else:
        result = whisper.transcribe(model, audio, beam_size=5, best_of=5, temperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0), language=WHISPER_LANGUAGE)
    
    # å°† result['language'] ä¿å­˜åˆ° output\log\transcript_language.jsonï¼Œæ ¼å¼å¦‚ {"language": "japanese"}
    os.makedirs('output/log', exist_ok=True)
    with open('output/log/transcript_language.json', 'w', encoding='utf-8') as f:
        json.dump({"language": result['language']}, f, ensure_ascii=False, indent=4)
    print(f"ğŸ“ å·²å°†è¯†åˆ«åˆ°çš„è¯­è¨€ä¿å­˜åˆ° output/log/transcript_language.json")

    # Process transcription results
    all_words: List[Dict[str, float]] = [
        {'text': f"{word['text']}", 'start': word['start'], 'end': word['end']}
        for segment in result['segments']
        for word in segment['words']
    ]
    
    df = pd.DataFrame(all_words)
    return df

def save_results(df: pd.DataFrame):
    # ğŸ’¾ Save transcription results as Excel and text files
    os.makedirs('output', exist_ok=True)
    os.makedirs('output/log', exist_ok=True)
    excel_path = os.path.join('output/log', "cleaned_chunks.xlsx")
    # ç»™df[text]åˆ—éƒ½åŠ ä¸Š""ï¼Œé˜²æ­¢æ•°å­—è¢«excelè‡ªåŠ¨è½¬æ¢ä¸ºæ•°å­—
    df['text'] = df['text'].apply(lambda x: f'"{x}"')
    df.to_excel(excel_path, index=False)
    print(f"ğŸ“Š Excelæ–‡ä»¶å·²ä¿å­˜åˆ° {excel_path}")

def get_whisper_language():
    try:
        with open("output/log/transcript_language.json", "r", encoding='utf-8') as f:
            language = json.load(f)["language"]
        return language
    except:
        print("æ— æ³•è¯»å–è¯­è¨€ä¿¡æ¯")
        return None

def transcribe(video_file: StopIteration):
    if not os.path.exists("output/log/cleaned_chunks.xlsx"):
        # ğŸ¥â¡ï¸ğŸ“ Transcribe video to text
        df = convert_video_to_audio_and_transcribe(video_file)
        if df is not None:
            save_results(df)
    else:
        print("ğŸ“Š è½¬å½•ç»“æœå·²å­˜åœ¨ï¼Œè·³è¿‡è½¬å½•æ­¥éª¤ã€‚")

if __name__ == "__main__":
    from core.step1_ytdlp import find_video_files
    video_file = find_video_files()
    print(f"ğŸ¬ æ‰¾åˆ°çš„è§†é¢‘æ–‡ä»¶: {video_file}, å¼€å§‹è½¬å½•...")
    transcribe(video_file)