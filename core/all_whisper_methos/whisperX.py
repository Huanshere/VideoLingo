import os
import sys
import whisperx
import torch
import pandas as pd
import json
from typing import Dict
import subprocess
import base64

sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from config import MODEL_DIR

def convert_video_to_audio(input_file: str) -> str:
    os.makedirs('output/audio', exist_ok=True)
    audio_file = 'output/audio/raw_full_audio.wav'
    
    if not os.path.exists(audio_file):
        ffmpeg_cmd = [
            'ffmpeg',
            '-i', input_file,
            '-vn',
            '-acodec', 'libmp3lame',
            '-ar', '16000',
            '-b:a', '64k',
            audio_file
        ]
        print(f"ğŸ¬â¡ï¸ğŸµ æ­£åœ¨è½¬æ¢ä¸ºéŸ³é¢‘......")
        subprocess.run(ffmpeg_cmd, check=True, stderr=subprocess.PIPE)
        print(f"ğŸ¬â¡ï¸ğŸµ å·²å°† <{input_file}> è½¬æ¢ä¸º <{audio_file}>\n")
    
    return audio_file

def encode_file_to_base64(file_path: str) -> str:
    print("ğŸ”„ æ­£åœ¨å°†éŸ³é¢‘æ–‡ä»¶ç¼–ç ä¸ºbase64...")
    with open(file_path, 'rb') as file:
        encoded = base64.b64encode(file.read()).decode('utf-8')
        print("âœ… æ–‡ä»¶å·²æˆåŠŸç¼–ç ä¸ºbase64")
        return encoded

def transcribe_audio(audio_file: str) -> Dict:
    from config import WHISPER_LANGUAGE
    device = "cuda" if torch.cuda.is_available() else "cpu"
    batch_size = 16  # å¦‚æœ GPU å†…å­˜ä¸è¶³ï¼Œè¯·å‡å°æ­¤å€¼
    compute_type = "float16"  # å¦‚æœ GPU å†…å­˜ä¸è¶³ï¼Œè¯·æ”¹ä¸º "int8"ï¼ˆå¯èƒ½ä¼šé™ä½å‡†ç¡®æ€§ï¼‰
    print(f"ğŸš€ æ­£åœ¨å¯åŠ¨WhisperX... è¯·è€å¿ƒç­‰å¾…...")
    try:
        whisperx_model_dir = os.path.join(MODEL_DIR, "whisperx")
        model = whisperx.load_model("large-v2", device, compute_type=compute_type, download_root=whisperx_model_dir)

        audio = whisperx.load_audio(audio_file)
        result = model.transcribe(audio, batch_size=batch_size, language=(None if WHISPER_LANGUAGE == 'auto' else WHISPER_LANGUAGE))
        # é‡Šæ”¾ GPU èµ„æº
        del model
        torch.cuda.empty_cache()
        
        # ä¿å­˜è¯­è¨€ä¿¡æ¯
        save_language(result['language'])

        # å¯¹é½ whisper è¾“å‡º
        model_a, metadata = whisperx.load_align_model(language_code=result["language"], device=device)
        result = whisperx.align(result["segments"], model_a, metadata, audio, device, return_char_alignments=False)

        # å†æ¬¡é‡Šæ”¾ GPU èµ„æº
        del model_a
        torch.cuda.empty_cache()

        return result
    except Exception as e:
        raise Exception(f"WhisperX å¤„ç†é”™è¯¯: {e}")

def process_transcription(result: Dict) -> pd.DataFrame:
    all_words = []
    for segment in result['segments']:
        for i, word in enumerate(segment['words']):
            if 'start' not in word and i > 0:
                all_words[-1]['text'] = f'{all_words[-1]["text"][:-1]}{word["word"]}"'
            else:
                word_dict = {
                    'text': f'{word["word"]}',
                    'start': word.get('start', all_words[-1]['end'] if all_words else 0),
                    'end': word['end'],
                    'score': word.get('score', 0)
                }
                all_words.append(word_dict)
    
    return pd.DataFrame(all_words)

def save_results(df: pd.DataFrame):
    os.makedirs('output/log', exist_ok=True)
    excel_path = os.path.join('output/log', "cleaned_chunks.xlsx")
    df['text'] = df['text'].apply(lambda x: f'"{x}"')
    df.to_excel(excel_path, index=False)
    print(f"ğŸ“Š Excelæ–‡ä»¶å·²ä¿å­˜åˆ° {excel_path}")

def save_language(language: str):
    os.makedirs('output/log', exist_ok=True)
    with open('output/log/transcript_language.json', 'w', encoding='utf-8') as f:
        json.dump({"language": language}, f, ensure_ascii=False, indent=4)

def transcribe(video_file: str):
    if not os.path.exists("output/log/cleaned_chunks.xlsx"):
        audio_file = convert_video_to_audio(video_file)
        
        if os.path.getsize(audio_file) > 25 * 1024 * 1024:
            print("âš ï¸ æ–‡ä»¶å¤§å°è¶…è¿‡25MBã€‚è¯·ä½¿ç”¨æ›´å°çš„æ–‡ä»¶ã€‚")
            return
        
        result = transcribe_audio(audio_file)
        
        df = process_transcription(result)
        save_results(df)
    else:
        print("ğŸ“Š è½¬å½•ç»“æœå·²å­˜åœ¨ï¼Œè·³è¿‡è½¬å½•æ­¥éª¤ã€‚")

if __name__ == "__main__":
    from core.step1_ytdlp import find_video_files
    video_file = find_video_files()
    print(f"ğŸ¬ æ‰¾åˆ°çš„è§†é¢‘æ–‡ä»¶: {video_file}, å¼€å§‹è½¬å½•...")
    transcribe(video_file)
